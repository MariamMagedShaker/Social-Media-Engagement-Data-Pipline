# ğŸ“Š Social Media Engagement Data Pipeline

This project builds a complete data pipeline to process, transform, and visualize social media engagement data using modern data tools.

---

## ğŸš€ Pipeline Overview

| Step | Tool      | Description                                 | Icon |
|------|-----------|---------------------------------------------|------|
| âœ… **Step 1** | **Kaggle**   | Public dataset used as source via Kaggle API | ğŸ¦ |
| âœ… **Step 2** | **Python**   | created Snowflake schema                     | ğŸ |
| âœ… **Step 3** | **Snowflake**| Loaded raw data into Snowflake tables        | â„ï¸ |
| âœ… **Step 4** | **dbt**      | Data transformation, staging, marts, and analysis models | ğŸ§± |
| âœ… **Step 5** | **Airflow**  | Scheduled dbt runs using DAGs                | ğŸŒ¬ï¸ |
| âœ… **Step 6** | **Power BI** | Built dashboards and insights                | ğŸ“Š |

### ğŸ§¬ Visual Diagram

![Pipeline](./assets/pipeline_diagram.png)

---

## ğŸ“¥ Data Source

- **Kaggle Dataset**: Downloaded using the [Kaggle Public API](https://www.kaggle.com/docs/api)

---

## ğŸ—ï¸ Tech Stack

- **Python**: schema creation
- **Snowflake**: Data warehousing
- **dbt**: Data transformation and modeling
- **Apache Airflow**: Workflow orchestration
- **Power BI**: Dashboarding and reporting

---

## ğŸ” Snowflake Connection (`profiles.yml`)

To connect `dbt` to Snowflake, create the `profiles.yml` file in the path:

```bash
~/.dbt/profiles.yml

your_profile_name:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <your_account>.snowflakecomputing.com
      user: <your_username>
      password: <your_password>
      role: <your_role>
      database: <your_database>
      warehouse: <your_warehouse>
      schema: <your_schema>
      threads: 4
      client_session_keep_alive: true

---

## ğŸ§  How to Run

```bash
# Setup Python env and install dependencies
python -m venv venv
source venv/bin/activate
pip install dbt-core dbt-snowflake    # install dbt core and dbt for snowflake 

# Run dbt transformations
cd DBT_snowflake_data
dbt run
dbt test     # to run the test
dbt docs generate     # dbt automatic generate for documentation
dbt docs serve --port 8088  # to view the documenation  

# Run Airflow for orchestration
pip install apache-airflow              
pip install apache-airflow-providers-snowflake 
export AIRFLOW_HOME=~/airflow           
mkdir -p ~/airflow/dags
nano ~/airflow/dags/dbt_dag.py       
airflow db migrate          
airflow standalone          #instead of airflow scheduler& airflow api-server

---   
## ğŸ“ˆ Visual Analysis

Power BI dashboards were created using marts and staging data from the dbt models. Below are some of the key visualizations:

- ğŸ“Š **Heatmap of Hourly Engagement by Day**  
- ğŸ’¬ **Post Sentiment Breakdown (Positive / Neutral / Negative)**  
- ğŸŒ **Average Interaction Rate by Platform Over Time**  
- ğŸŒ **Geographical Engagement by Country**  
- ğŸ” **Virality Trend by Month and Year**
- ğŸ”¢ **KPI Tiles for Total Posts, Likes, Shares, and Comments**

---

## ğŸ“Š Dashboards

### ğŸ“Œ Dashboard Page 1: Campaign, Sentiment, and Time Insights

![Dashboard 1](./assets/page_1.PNG)

### ğŸ“Œ Dashboard Page 2: Global Metrics and Virality Trends

![Dashboard 2](./assets/page_2.PNG)

